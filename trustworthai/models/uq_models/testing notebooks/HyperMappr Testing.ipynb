{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52e06e20-3ffa-4954-be62-75d8dc07e6f3",
   "metadata": {},
   "source": [
    "# HyperMappr Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ad47e2-0571-4b8a-be15-78637fd22776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from trustworthai.models.uq_models.drop_UNet import normalization_layer\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bbf53de-6f90-449d-893d-78d73da28fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various dropout and dropconnect layers\n",
    "from trustworthai.models.uq_models.uq_layers.dropoutconnect import (\n",
    "    UQDropout,\n",
    "    UQDropout2d,\n",
    "    UQDropout3d,\n",
    "    UQGaussianDropout,\n",
    "    UQGaussianDropout2d,\n",
    "    UQGaussianDropout3d,\n",
    "    UQDropConnect,\n",
    "    UQDropConnect2d,\n",
    "    UQDropConnect3d,\n",
    "    UQGaussianConnect,\n",
    "    UQGaussianConnect2d,\n",
    "    UQGaussianConnect3d,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9649ba0e-3974-4830-bb25-9a67bbd274bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n- what is the kernel size for their deconv block? ive put three\\n- what is their l_relu parameter? I have put 0.01 (todo make as a gloabl const)\\n- what do they do about the output shape, do they upsample or no its strange\\n\\n'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_conv_func(dims, transpose=False):\n",
    "    # determine convolution func\n",
    "        if dims == 2:\n",
    "            if transpose:\n",
    "                return nn.ConvTranspose2d\n",
    "            else:\n",
    "                return nn.Conv2d\n",
    "        elif dims == 3:\n",
    "            if transpose:\n",
    "                return nn.ConvTranspose3d\n",
    "            else:\n",
    "                return nn.Conv3d\n",
    "        else:\n",
    "            raise ValueError(f\"values of dims of 2 or 3 (2D or 2D conv) are supported only, not {dims}\")\n",
    "\n",
    "\n",
    "class HM3Block(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 dims=2, # 2 =2D, 3=3D,\n",
    "                 kernel_size=3,\n",
    "                 dropout_type=\"bernoulli\",\n",
    "                 dropout_p=0.1,\n",
    "                 gaussout_mean=1, # NOTE THE PREDICT STEP CURRENTLY ONLY SUPPORTS MEAN = 1\n",
    "                 dropconnect_type=\"bernoulli\",\n",
    "                 dropconnect_p=0.1,\n",
    "                 gaussconnect_mean=1,\n",
    "                 norm_type=\"bn\", # batch norm, or instance 'in' or group 'gn'\n",
    "                 use_multidim_dropout = True, # use 2d or 3d dropout instead of 1d dropout. applies to gaussian dropout too\n",
    "                 use_multidim_dropconnect = True, # use 2d or 3d dropconnect instead of 1d dropconnect, applies to gaussian dropconnect too\n",
    "                 groups=1,\n",
    "                 gn_groups=4, # number of groups for group norm normalization.\n",
    "                 uq_layer_on_conv2=False,\n",
    "                 res_block=True\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # determine convolution func\n",
    "        conv_f = get_conv_func(dims, transpose=False)\n",
    "            \n",
    "        # determine dropout func\n",
    "        if dropout_type:\n",
    "            # standard dropout\n",
    "            if dropout_type == \"bernoulli\":\n",
    "                if use_multidim_dropout:\n",
    "                    if dims == 2:\n",
    "                        dropout_f = UQDropout2d\n",
    "                    else:\n",
    "                        dropout_f = UQDropout3d\n",
    "                else:\n",
    "                    dropout_f = UQDropout\n",
    "                    \n",
    "            # gaussian dropout    \n",
    "            elif dropout_type == \"gaussian\":\n",
    "                if use_multidim_dropout:\n",
    "                    if dims == 2:\n",
    "                        dropout_f = UQGaussianDropout2d\n",
    "                    elif dims == 3:\n",
    "                        dropout_f = UQGaussianDropout3d\n",
    "                else:\n",
    "                    dropout_f = UQGaussianDropout\n",
    "            else:\n",
    "                raise ValueError(f\"dropout type {dropout_type} not supported, \"\n",
    "                                 \"only 'bernoulli' or 'gaussian' are supported\")\n",
    "        # no dropout\n",
    "        else:\n",
    "            dropout_f = None\n",
    "        \n",
    "        # determine dropconnect function\n",
    "        if dropconnect_type:\n",
    "            # standard dropconnect\n",
    "            if dropconnect_type == \"bernoulli\":\n",
    "                if use_multidim_dropout:\n",
    "                    if dims == 2:\n",
    "                        dropconnect_f = UQDropConnect2d\n",
    "                    else:\n",
    "                        dropconnect_f = UQDropConnect3d\n",
    "                else:\n",
    "                    dropconnect_f = UQDropConnect\n",
    "                    \n",
    "            # gaussian dropout    \n",
    "            elif dropconnect_type == \"gaussian\":\n",
    "                if use_multidim_dropconnect:\n",
    "                    if dims == 2:\n",
    "                        dropconnect_f = UQGaussianConnect2d\n",
    "                    elif dims == 3:\n",
    "                        dropconnect_f = UQGaussianConnect3d\n",
    "                else:\n",
    "                    dropconnect_f = UQGaussianConnect\n",
    "            else:\n",
    "                raise ValueError(f\"dropconnect type {dropconnect_type} not supported, \"\n",
    "                                 \"only 'bernoulli' or 'gaussian' are supported\")\n",
    "        else:\n",
    "            dropconnect_f = None\n",
    "        \n",
    "        self.uq_layers = []\n",
    "\n",
    "        # layers needed for the forward pass\n",
    "        self.conv1 = conv_f(in_channels, out_channels, kernel_size, padding=2, bias=False, dilation=2)\n",
    "        if dropconnect_f:\n",
    "            if dropconnect_type == \"bernoulli\":\n",
    "                self.convout1 = dropconnect_f(self.conv1, None, dropconnect_p)\n",
    "            else:\n",
    "                self.convout1 = dropconnect_f(self.conv1, None, gaussconnect_mean, dropconnect_p)\n",
    "            self.uq_layers.append(self.convout1)\n",
    "        else:\n",
    "            self.convout1 = self.conv1\n",
    "\n",
    "        if dropout_f:\n",
    "            if dropout_type == \"bernoulli\":\n",
    "                self.dropout1 = dropout_f(dropout_p)\n",
    "            else:\n",
    "                self.dropout1 = dropout_f(gaussout_mean, dropout_p)\n",
    "            self.uq_layers.append(self.dropout1)\n",
    "        else:\n",
    "            self.dropout1 = None\n",
    "\n",
    "        self.norm1 = normalization_layer(in_channels, norm=norm_type, gn_groups=gn_groups, dims=dims)()\n",
    "\n",
    "        self.conv2 = conv_f(out_channels, out_channels, kernel_size, padding=2, bias=False, dilation=2)\n",
    "        if dropconnect_f and uq_layer_on_conv2:\n",
    "            if dropconnect_type == \"bernoulli\":\n",
    "                self.convout2 = dropconnect_f(self.conv2, None, dropconnect_p)\n",
    "            else:\n",
    "                self.convout2 = dropconnect_f(self.conv2, None, gaussconnect_mean, dropconnect_p)\n",
    "            self.uq_layers.append(self.convout2)\n",
    "        else:\n",
    "            self.convout2 = self.conv2\n",
    "\n",
    "        if dropout_f and uq_layer_on_conv2:\n",
    "            if dropout_type == \"bernoulli\":\n",
    "                self.dropout2 = dropout_f(dropout_p)\n",
    "            else:\n",
    "                self.dropout2 = dropout_f(gaussout_mean, dropout_p)\n",
    "            self.uq_layers.append(self.dropout1)\n",
    "        else:\n",
    "            self.dropout2 = None\n",
    "\n",
    "        self.norm2 = normalization_layer(out_channels, norm=norm_type, gn_groups=gn_groups, dims=dims)()\n",
    "\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.res_block = res_block\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print()\n",
    "        print(\"Res UQ Block\")\n",
    "        print(\"in shape: \", x.shape)\n",
    "        out = x\n",
    "        out = self.norm1(out)\n",
    "        out = self.lrelu(out)\n",
    "        out = self.convout1(out)\n",
    "        print(\"conv 1 out shape: \", out.shape)\n",
    "        if self.dropout1:\n",
    "            out = self.dropout1(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.lrelu(out)\n",
    "        out = self.convout2(out)\n",
    "        print(\"conv 2 out shape: \", out.shape)\n",
    "        \n",
    "        if self.res_block:\n",
    "            out = torch.add(out, x)\n",
    "        print(\"res out shape: \", out.shape)\n",
    "        print(\"================================\")\n",
    "        return out\n",
    "    \n",
    "    def set_applyfunc(self, a):\n",
    "        for l in self.uq_layers:\n",
    "            l.set_applyfunc(a)\n",
    "            \n",
    "            \n",
    "class HMFeatureBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dims):\n",
    "        super().__init__()\n",
    "        \n",
    "        conv_func = get_conv_func(dims, transpose=False)\n",
    "        norm_func = normalization_layer(out_channels, norm='in', dims=dims)\n",
    "        \n",
    "        self.conv1 = conv_func(in_channels, out_channels, kernel_size=3, dilation=2, padding=2)\n",
    "        self.norm = norm_func()\n",
    "        self.lrelu = nn.LeakyReLU(0.01)\n",
    "        self.conv2 = conv_func(out_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print()\n",
    "        print(\"Feature Block\")\n",
    "        print(\"in shape: \", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        print(\"conv 1 out shape: \", x.shape)\n",
    "        x = self.norm(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.conv2(x)\n",
    "        print(\"conv 2 out shape: \", x.shape)\n",
    "        print(\"================================\")\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class HMUpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dims):\n",
    "        super().__init__()\n",
    "        \n",
    "        # determine convolution func\n",
    "        conv_func = get_conv_func(dims, transpose=True)\n",
    "        \n",
    "        self.norm1 = normalization_layer(in_channels, norm='in', dims=dims)()\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.up_conv = conv_func(in_channels, out_channels, kernel_size=3, padding=1, output_padding=1, stride=2)\n",
    "        self.norm2 = normalization_layer(out_channels, norm='in', dims=dims)()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print()\n",
    "        print(\"Upsample Block\")\n",
    "        print(\"in shape: \", x.shape)\n",
    "        x = self.norm1(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.up_conv(x)\n",
    "        print(\"conv 1 out shape: \", x.shape)\n",
    "        x = self.norm2(x)\n",
    "        x = self.lrelu(x)\n",
    "        \n",
    "        print(\"================================\")\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class HyperMapp3r(nn.Module):\n",
    "    def __init__(self, dims=3,\n",
    "                 in_channels=3,\n",
    "                 out_channels=1,\n",
    "                 encoder_features=[16, 32, 64, 128, 256],\n",
    "                 decoder_features=[128, 64, 32, 16],\n",
    "                 softmax=True,\n",
    "                 block_params={\n",
    "                     \"dropout_type\":\"bernoulli\",\n",
    "                     \"dropout_p\":0.1,\n",
    "                     \"gaussout_mean\":None, \n",
    "                     \"dropconnect_type\":None,\n",
    "                     \"dropconnect_p\":None,\n",
    "                     \"gaussconnect_mean\":None,\n",
    "                     \"norm_type\":\"in\", \n",
    "                     \"use_multidim_dropout\":True, \n",
    "                     \"use_multidim_dropconnect\":True, \n",
    "                     \"uq_layer_on_conv2\":False,\n",
    "                 }):\n",
    "        super().__init__()\n",
    "        \n",
    "        print(\"dims: \", dims)\n",
    "        \n",
    "        conv_func = get_conv_func(dims, transpose=False)\n",
    "        print(\"conv func: \", conv_func)\n",
    "        \n",
    "        self.encoder_resuq_blocks = nn.ModuleList([\n",
    "            HM3Block(fs, fs, dims, **block_params)\n",
    "            for fs in encoder_features\n",
    "        ])\n",
    "        self.encoder_down_blocks = nn.ModuleList([\n",
    "            conv_func(ins, outs, kernel_size=3, stride=2, padding=1)\n",
    "            for (ins, outs) in zip([in_channels] + encoder_features[:-1], encoder_features)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_feature_blocks = nn.ModuleList([\n",
    "            HMFeatureBlock(ins, outs, dims)\n",
    "            for (ins, outs) in zip([f * 2 for f in decoder_features[:-1]], decoder_features[:-1])\n",
    "        ])\n",
    "        \n",
    "        self.decoder_upsample_blocks = nn.ModuleList([\n",
    "            HMUpsampleBlock(ins, outs, dims)\n",
    "            for (ins, outs) in zip([f * 2 for f in decoder_features], decoder_features)\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        self.skip_final_convs = nn.ModuleList([\n",
    "            conv_func(fs, out_channels, kernel_size=1)\n",
    "            for fs in decoder_features[1:-1]\n",
    "        ])\n",
    "        \n",
    "        final_a_features = encoder_features[0] * 2\n",
    "        print(\"final a features: \", final_a_features)\n",
    "        self.final_a = conv_func(final_a_features, final_a_features, kernel_size=3, stride=1, padding=1)\n",
    "        print(\"final a weight size: \", self.final_a.weight.shape)\n",
    "        self.final_b = conv_func(final_a_features, out_channels, kernel_size=1)\n",
    "        \n",
    "        self.lrelu = nn.LeakyReLU(0.01)\n",
    "        mode = \"bilinear\" if dims == 2 else \"trilinear\"\n",
    "        self.interpolate = lambda x : F.interpolate(x, scale_factor=2, mode=mode)\n",
    "        self.softmax = nn.Softmax(dim=1) if softmax else None\n",
    "        \n",
    "        \n",
    "        self.down_steps = len(self.encoder_down_blocks)\n",
    "        self.up_steps = len(self.decoder_upsample_blocks)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_conns = []\n",
    "        out = x\n",
    "        \n",
    "        print(\"hypermappr3\")\n",
    "        print(\"in shape: \", x.shape)\n",
    "        print(\"~~ENCODER~~\")\n",
    "        # encoder path\n",
    "        for l in range(self.down_steps):\n",
    "            out = self.encoder_down_blocks[l](out)\n",
    "            out = self.encoder_resuq_blocks[l](out)\n",
    "            print(\"encoder group out shape\", out.shape)\n",
    "            \n",
    "            if l != self.down_steps-1:\n",
    "                skip_conns.append(out)\n",
    "                \n",
    "        # decoder path\n",
    "        print(\"~~DECODER~~\")\n",
    "        out = self.decoder_upsample_blocks[0](out)\n",
    "        secondary_skip_conns = []\n",
    "        for l in range(1, self.up_steps):\n",
    "            print(\"decoder group in: \", out.shape)\n",
    "            print(\"skip conn shape: \", skip_conns[-1].shape)\n",
    "            out = torch.cat([out, skip_conns.pop()], dim=1)\n",
    "            print(\"post cat shape: \", out.shape)\n",
    "            out = self.decoder_feature_blocks[l-1](out)\n",
    "            out = self.decoder_upsample_blocks[l](out)\n",
    "            \n",
    "            if l >= 1:\n",
    "                secondary_skip_conns.append(out)\n",
    "        \n",
    "        print(\"final cat in shape: \", out.shape)\n",
    "        out = torch.cat([out, skip_conns.pop()], dim=1)\n",
    "        print(\"post cat shape: \", out.shape)\n",
    "        out = self.final_a(out)\n",
    "        out = self.lrelu(out)\n",
    "        out = self.final_b(out)\n",
    "        print(\"main branch otu shape: \", out.shape)\n",
    "        \n",
    "        # combine secondary skips\n",
    "        sk1 = self.skip_final_convs[0](secondary_skip_conns[0])\n",
    "        print(\"sk1 out shape pre interpolate: \", sk1.shape)\n",
    "        sk1 = self.interpolate(sk1)\n",
    "        print(\"sk1 out shape post interpolate: \", sk1.shape)\n",
    "        sk2 = self.skip_final_convs[1](secondary_skip_conns[1])\n",
    "        print(\"sk2 out shape pre interpolate: \", sk2.shape)\n",
    "        sk2 = torch.add(sk1, sk2)\n",
    "        print(\"sk2 out shape post add: \", sk2.shape)\n",
    "        sk2 = self.interpolate(sk2)\n",
    "        print(\"sk2 out shape post interpolate: \", sk2.shape)\n",
    "        \n",
    "        out = torch.add(out, sk2)\n",
    "        \n",
    "        out = self.interpolate(out)\n",
    "        \n",
    "        if self.softmax:\n",
    "            out = self.softmax(out)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "\n",
    "- what is the kernel size for their deconv block? ive put three\n",
    "- what is their l_relu parameter? I have put 0.01 (todo make as a gloabl const)\n",
    "- what do they do about the output shape, do they upsample or no its strange\n",
    "- I think its not great the way they do the upsampling at the last layer, would be better\n",
    "- to have a neural net layer do the upscale I think...\n",
    "- need to try and use the kernel sizes given in the paper as well (they have a few 7x7 ones...\n",
    "\"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "841b5528-a993-40a0-aa8e-72be1e166ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims:  3\n",
      "conv func:  <class 'torch.nn.modules.conv.Conv3d'>\n",
      "final a features:  32\n",
      "final a weight size:  torch.Size([32, 32, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "model = HyperMapp3r(dims=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5516200c-5deb-4dec-9a9d-13a7807ef961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypermappr3\n",
      "in shape:  torch.Size([16, 3, 32, 224, 224])\n",
      "~~ENCODER~~\n",
      "\n",
      "Res UQ Block\n",
      "in shape:  torch.Size([16, 16, 16, 112, 112])\n",
      "conv 1 out shape:  torch.Size([16, 16, 16, 112, 112])\n",
      "conv 2 out shape:  torch.Size([16, 16, 16, 112, 112])\n",
      "res out shape:  torch.Size([16, 16, 16, 112, 112])\n",
      "================================\n",
      "encoder group out shape torch.Size([16, 16, 16, 112, 112])\n",
      "\n",
      "Res UQ Block\n",
      "in shape:  torch.Size([16, 32, 8, 56, 56])\n",
      "conv 1 out shape:  torch.Size([16, 32, 8, 56, 56])\n",
      "conv 2 out shape:  torch.Size([16, 32, 8, 56, 56])\n",
      "res out shape:  torch.Size([16, 32, 8, 56, 56])\n",
      "================================\n",
      "encoder group out shape torch.Size([16, 32, 8, 56, 56])\n",
      "\n",
      "Res UQ Block\n",
      "in shape:  torch.Size([16, 64, 4, 28, 28])\n",
      "conv 1 out shape:  torch.Size([16, 64, 4, 28, 28])\n",
      "conv 2 out shape:  torch.Size([16, 64, 4, 28, 28])\n",
      "res out shape:  torch.Size([16, 64, 4, 28, 28])\n",
      "================================\n",
      "encoder group out shape torch.Size([16, 64, 4, 28, 28])\n",
      "\n",
      "Res UQ Block\n",
      "in shape:  torch.Size([16, 128, 2, 14, 14])\n",
      "conv 1 out shape:  torch.Size([16, 128, 2, 14, 14])\n",
      "conv 2 out shape:  torch.Size([16, 128, 2, 14, 14])\n",
      "res out shape:  torch.Size([16, 128, 2, 14, 14])\n",
      "================================\n",
      "encoder group out shape torch.Size([16, 128, 2, 14, 14])\n",
      "\n",
      "Res UQ Block\n",
      "in shape:  torch.Size([16, 256, 1, 7, 7])\n",
      "conv 1 out shape:  torch.Size([16, 256, 1, 7, 7])\n",
      "conv 2 out shape:  torch.Size([16, 256, 1, 7, 7])\n",
      "res out shape:  torch.Size([16, 256, 1, 7, 7])\n",
      "================================\n",
      "encoder group out shape torch.Size([16, 256, 1, 7, 7])\n",
      "~~DECODER~~\n",
      "\n",
      "Upsample Block\n",
      "in shape:  torch.Size([16, 256, 1, 7, 7])\n",
      "conv 1 out shape:  torch.Size([16, 128, 2, 14, 14])\n",
      "================================\n",
      "decoder group in:  torch.Size([16, 128, 2, 14, 14])\n",
      "skip conn shape:  torch.Size([16, 128, 2, 14, 14])\n",
      "post cat shape:  torch.Size([16, 256, 2, 14, 14])\n",
      "\n",
      "Feature Block\n",
      "in shape:  torch.Size([16, 256, 2, 14, 14])\n",
      "conv 1 out shape:  torch.Size([16, 128, 2, 14, 14])\n",
      "conv 2 out shape:  torch.Size([16, 128, 2, 14, 14])\n",
      "================================\n",
      "\n",
      "Upsample Block\n",
      "in shape:  torch.Size([16, 128, 2, 14, 14])\n",
      "conv 1 out shape:  torch.Size([16, 64, 4, 28, 28])\n",
      "================================\n",
      "decoder group in:  torch.Size([16, 64, 4, 28, 28])\n",
      "skip conn shape:  torch.Size([16, 64, 4, 28, 28])\n",
      "post cat shape:  torch.Size([16, 128, 4, 28, 28])\n",
      "\n",
      "Feature Block\n",
      "in shape:  torch.Size([16, 128, 4, 28, 28])\n",
      "conv 1 out shape:  torch.Size([16, 64, 4, 28, 28])\n",
      "conv 2 out shape:  torch.Size([16, 64, 4, 28, 28])\n",
      "================================\n",
      "\n",
      "Upsample Block\n",
      "in shape:  torch.Size([16, 64, 4, 28, 28])\n",
      "conv 1 out shape:  torch.Size([16, 32, 8, 56, 56])\n",
      "================================\n",
      "decoder group in:  torch.Size([16, 32, 8, 56, 56])\n",
      "skip conn shape:  torch.Size([16, 32, 8, 56, 56])\n",
      "post cat shape:  torch.Size([16, 64, 8, 56, 56])\n",
      "\n",
      "Feature Block\n",
      "in shape:  torch.Size([16, 64, 8, 56, 56])\n",
      "conv 1 out shape:  torch.Size([16, 32, 8, 56, 56])\n",
      "conv 2 out shape:  torch.Size([16, 32, 8, 56, 56])\n",
      "================================\n",
      "\n",
      "Upsample Block\n",
      "in shape:  torch.Size([16, 32, 8, 56, 56])\n",
      "conv 1 out shape:  torch.Size([16, 16, 16, 112, 112])\n",
      "================================\n",
      "final cat in shape:  torch.Size([16, 16, 16, 112, 112])\n",
      "post cat shape:  torch.Size([16, 32, 16, 112, 112])\n",
      "main branch otu shape:  torch.Size([16, 1, 16, 112, 112])\n",
      "sk1 out shape pre interpolate:  torch.Size([16, 1, 4, 28, 28])\n",
      "sk1 out shape post interpolate:  torch.Size([16, 1, 8, 56, 56])\n",
      "sk2 out shape pre interpolate:  torch.Size([16, 1, 8, 56, 56])\n",
      "sk2 out shape post add:  torch.Size([16, 1, 8, 56, 56])\n",
      "sk2 out shape post interpolate:  torch.Size([16, 1, 16, 112, 112])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "HyperMapp3r                              --                        --\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "├─ModuleList: 1-5                        --                        --\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Conv3d: 2-1                       [16, 16, 16, 112, 112]    1,312\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─HM3Block: 2-2                     [16, 16, 16, 112, 112]    --\n",
       "│    │    └─InstanceNorm3d: 3-1          [16, 16, 16, 112, 112]    --\n",
       "│    │    └─LeakyReLU: 3-2               [16, 16, 16, 112, 112]    --\n",
       "│    │    └─Conv3d: 3-3                  [16, 16, 16, 112, 112]    6,912\n",
       "│    │    └─UQDropout3d: 3-4             [16, 16, 16, 112, 112]    --\n",
       "│    │    └─InstanceNorm3d: 3-5          [16, 16, 16, 112, 112]    --\n",
       "│    │    └─LeakyReLU: 3-6               [16, 16, 16, 112, 112]    --\n",
       "│    │    └─Conv3d: 3-7                  [16, 16, 16, 112, 112]    6,912\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Conv3d: 2-3                       [16, 32, 8, 56, 56]       13,856\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─HM3Block: 2-4                     [16, 32, 8, 56, 56]       --\n",
       "│    │    └─InstanceNorm3d: 3-8          [16, 32, 8, 56, 56]       --\n",
       "│    │    └─LeakyReLU: 3-9               [16, 32, 8, 56, 56]       --\n",
       "│    │    └─Conv3d: 3-10                 [16, 32, 8, 56, 56]       27,648\n",
       "│    │    └─UQDropout3d: 3-11            [16, 32, 8, 56, 56]       --\n",
       "│    │    └─InstanceNorm3d: 3-12         [16, 32, 8, 56, 56]       --\n",
       "│    │    └─LeakyReLU: 3-13              [16, 32, 8, 56, 56]       --\n",
       "│    │    └─Conv3d: 3-14                 [16, 32, 8, 56, 56]       27,648\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Conv3d: 2-5                       [16, 64, 4, 28, 28]       55,360\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─HM3Block: 2-6                     [16, 64, 4, 28, 28]       --\n",
       "│    │    └─InstanceNorm3d: 3-15         [16, 64, 4, 28, 28]       --\n",
       "│    │    └─LeakyReLU: 3-16              [16, 64, 4, 28, 28]       --\n",
       "│    │    └─Conv3d: 3-17                 [16, 64, 4, 28, 28]       110,592\n",
       "│    │    └─UQDropout3d: 3-18            [16, 64, 4, 28, 28]       --\n",
       "│    │    └─InstanceNorm3d: 3-19         [16, 64, 4, 28, 28]       --\n",
       "│    │    └─LeakyReLU: 3-20              [16, 64, 4, 28, 28]       --\n",
       "│    │    └─Conv3d: 3-21                 [16, 64, 4, 28, 28]       110,592\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Conv3d: 2-7                       [16, 128, 2, 14, 14]      221,312\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─HM3Block: 2-8                     [16, 128, 2, 14, 14]      --\n",
       "│    │    └─InstanceNorm3d: 3-22         [16, 128, 2, 14, 14]      --\n",
       "│    │    └─LeakyReLU: 3-23              [16, 128, 2, 14, 14]      --\n",
       "│    │    └─Conv3d: 3-24                 [16, 128, 2, 14, 14]      442,368\n",
       "│    │    └─UQDropout3d: 3-25            [16, 128, 2, 14, 14]      --\n",
       "│    │    └─InstanceNorm3d: 3-26         [16, 128, 2, 14, 14]      --\n",
       "│    │    └─LeakyReLU: 3-27              [16, 128, 2, 14, 14]      --\n",
       "│    │    └─Conv3d: 3-28                 [16, 128, 2, 14, 14]      442,368\n",
       "├─ModuleList: 1-2                        --                        --\n",
       "│    └─Conv3d: 2-9                       [16, 256, 1, 7, 7]        884,992\n",
       "├─ModuleList: 1-1                        --                        --\n",
       "│    └─HM3Block: 2-10                    [16, 256, 1, 7, 7]        --\n",
       "│    │    └─InstanceNorm3d: 3-29         [16, 256, 1, 7, 7]        --\n",
       "│    │    └─LeakyReLU: 3-30              [16, 256, 1, 7, 7]        --\n",
       "│    │    └─Conv3d: 3-31                 [16, 256, 1, 7, 7]        1,769,472\n",
       "│    │    └─UQDropout3d: 3-32            [16, 256, 1, 7, 7]        --\n",
       "│    │    └─InstanceNorm3d: 3-33         [16, 256, 1, 7, 7]        --\n",
       "│    │    └─LeakyReLU: 3-34              [16, 256, 1, 7, 7]        --\n",
       "│    │    └─Conv3d: 3-35                 [16, 256, 1, 7, 7]        1,769,472\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─HMUpsampleBlock: 2-11             [16, 128, 2, 14, 14]      --\n",
       "│    │    └─InstanceNorm3d: 3-36         [16, 256, 1, 7, 7]        --\n",
       "│    │    └─LeakyReLU: 3-37              [16, 256, 1, 7, 7]        --\n",
       "│    │    └─ConvTranspose3d: 3-38        [16, 128, 2, 14, 14]      884,864\n",
       "│    │    └─InstanceNorm3d: 3-39         [16, 128, 2, 14, 14]      --\n",
       "│    │    └─LeakyReLU: 3-40              [16, 128, 2, 14, 14]      --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─HMFeatureBlock: 2-12              [16, 128, 2, 14, 14]      --\n",
       "│    │    └─Conv3d: 3-41                 [16, 128, 2, 14, 14]      884,864\n",
       "│    │    └─InstanceNorm3d: 3-42         [16, 128, 2, 14, 14]      --\n",
       "│    │    └─LeakyReLU: 3-43              [16, 128, 2, 14, 14]      --\n",
       "│    │    └─Conv3d: 3-44                 [16, 128, 2, 14, 14]      16,512\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─HMUpsampleBlock: 2-13             [16, 64, 4, 28, 28]       --\n",
       "│    │    └─InstanceNorm3d: 3-45         [16, 128, 2, 14, 14]      --\n",
       "│    │    └─LeakyReLU: 3-46              [16, 128, 2, 14, 14]      --\n",
       "│    │    └─ConvTranspose3d: 3-47        [16, 64, 4, 28, 28]       221,248\n",
       "│    │    └─InstanceNorm3d: 3-48         [16, 64, 4, 28, 28]       --\n",
       "│    │    └─LeakyReLU: 3-49              [16, 64, 4, 28, 28]       --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─HMFeatureBlock: 2-14              [16, 64, 4, 28, 28]       --\n",
       "│    │    └─Conv3d: 3-50                 [16, 64, 4, 28, 28]       221,248\n",
       "│    │    └─InstanceNorm3d: 3-51         [16, 64, 4, 28, 28]       --\n",
       "│    │    └─LeakyReLU: 3-52              [16, 64, 4, 28, 28]       --\n",
       "│    │    └─Conv3d: 3-53                 [16, 64, 4, 28, 28]       4,160\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─HMUpsampleBlock: 2-15             [16, 32, 8, 56, 56]       --\n",
       "│    │    └─InstanceNorm3d: 3-54         [16, 64, 4, 28, 28]       --\n",
       "│    │    └─LeakyReLU: 3-55              [16, 64, 4, 28, 28]       --\n",
       "│    │    └─ConvTranspose3d: 3-56        [16, 32, 8, 56, 56]       55,328\n",
       "│    │    └─InstanceNorm3d: 3-57         [16, 32, 8, 56, 56]       --\n",
       "│    │    └─LeakyReLU: 3-58              [16, 32, 8, 56, 56]       --\n",
       "├─ModuleList: 1-3                        --                        --\n",
       "│    └─HMFeatureBlock: 2-16              [16, 32, 8, 56, 56]       --\n",
       "│    │    └─Conv3d: 3-59                 [16, 32, 8, 56, 56]       55,328\n",
       "│    │    └─InstanceNorm3d: 3-60         [16, 32, 8, 56, 56]       --\n",
       "│    │    └─LeakyReLU: 3-61              [16, 32, 8, 56, 56]       --\n",
       "│    │    └─Conv3d: 3-62                 [16, 32, 8, 56, 56]       1,056\n",
       "├─ModuleList: 1-4                        --                        --\n",
       "│    └─HMUpsampleBlock: 2-17             [16, 16, 16, 112, 112]    --\n",
       "│    │    └─InstanceNorm3d: 3-63         [16, 32, 8, 56, 56]       --\n",
       "│    │    └─LeakyReLU: 3-64              [16, 32, 8, 56, 56]       --\n",
       "│    │    └─ConvTranspose3d: 3-65        [16, 16, 16, 112, 112]    13,840\n",
       "│    │    └─InstanceNorm3d: 3-66         [16, 16, 16, 112, 112]    --\n",
       "│    │    └─LeakyReLU: 3-67              [16, 16, 16, 112, 112]    --\n",
       "├─Conv3d: 1-6                            [16, 32, 16, 112, 112]    27,680\n",
       "├─LeakyReLU: 1-7                         [16, 32, 16, 112, 112]    --\n",
       "├─Conv3d: 1-8                            [16, 1, 16, 112, 112]     33\n",
       "├─ModuleList: 1-5                        --                        --\n",
       "│    └─Conv3d: 2-18                      [16, 1, 4, 28, 28]        65\n",
       "│    └─Conv3d: 2-19                      [16, 1, 8, 56, 56]        33\n",
       "├─Softmax: 1-9                           [16, 1, 32, 224, 224]     --\n",
       "==========================================================================================\n",
       "Total params: 8,277,075\n",
       "Trainable params: 8,277,075\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 312.56\n",
       "==========================================================================================\n",
       "Input size (MB): 308.28\n",
       "Forward/backward pass size (MB): 3309.61\n",
       "Params size (MB): 33.11\n",
       "Estimated Total Size (MB): 3651.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (16, 3, 32, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "566453a7-bd73-4854-bf42-217238c9c61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "HyperMapp3r                              --\n",
       "├─ModuleList: 1-1                        --\n",
       "│    └─HM3Block: 2-1                     --\n",
       "│    │    └─Conv2d: 3-1                  2,304\n",
       "│    │    └─Conv2d: 3-2                  (recursive)\n",
       "│    │    └─UQDropout2d: 3-3             --\n",
       "│    │    └─InstanceNorm2d: 3-4          --\n",
       "│    │    └─Conv2d: 3-5                  2,304\n",
       "│    │    └─Conv2d: 3-6                  (recursive)\n",
       "│    │    └─InstanceNorm2d: 3-7          --\n",
       "│    │    └─LeakyReLU: 3-8               --\n",
       "│    └─HM3Block: 2-2                     --\n",
       "│    │    └─Conv2d: 3-9                  9,216\n",
       "│    │    └─Conv2d: 3-10                 (recursive)\n",
       "│    │    └─UQDropout2d: 3-11            --\n",
       "│    │    └─InstanceNorm2d: 3-12         --\n",
       "│    │    └─Conv2d: 3-13                 9,216\n",
       "│    │    └─Conv2d: 3-14                 (recursive)\n",
       "│    │    └─InstanceNorm2d: 3-15         --\n",
       "│    │    └─LeakyReLU: 3-16              --\n",
       "│    └─HM3Block: 2-3                     --\n",
       "│    │    └─Conv2d: 3-17                 36,864\n",
       "│    │    └─Conv2d: 3-18                 (recursive)\n",
       "│    │    └─UQDropout2d: 3-19            --\n",
       "│    │    └─InstanceNorm2d: 3-20         --\n",
       "│    │    └─Conv2d: 3-21                 36,864\n",
       "│    │    └─Conv2d: 3-22                 (recursive)\n",
       "│    │    └─InstanceNorm2d: 3-23         --\n",
       "│    │    └─LeakyReLU: 3-24              --\n",
       "│    └─HM3Block: 2-4                     --\n",
       "│    │    └─Conv2d: 3-25                 147,456\n",
       "│    │    └─Conv2d: 3-26                 (recursive)\n",
       "│    │    └─UQDropout2d: 3-27            --\n",
       "│    │    └─InstanceNorm2d: 3-28         --\n",
       "│    │    └─Conv2d: 3-29                 147,456\n",
       "│    │    └─Conv2d: 3-30                 (recursive)\n",
       "│    │    └─InstanceNorm2d: 3-31         --\n",
       "│    │    └─LeakyReLU: 3-32              --\n",
       "│    └─HM3Block: 2-5                     --\n",
       "│    │    └─Conv2d: 3-33                 589,824\n",
       "│    │    └─Conv2d: 3-34                 (recursive)\n",
       "│    │    └─UQDropout2d: 3-35            --\n",
       "│    │    └─InstanceNorm2d: 3-36         --\n",
       "│    │    └─Conv2d: 3-37                 589,824\n",
       "│    │    └─Conv2d: 3-38                 (recursive)\n",
       "│    │    └─InstanceNorm2d: 3-39         --\n",
       "│    │    └─LeakyReLU: 3-40              --\n",
       "├─ModuleList: 1-2                        --\n",
       "│    └─Conv2d: 2-6                       448\n",
       "│    └─Conv2d: 2-7                       4,640\n",
       "│    └─Conv2d: 2-8                       18,496\n",
       "│    └─Conv2d: 2-9                       73,856\n",
       "│    └─Conv2d: 2-10                      295,168\n",
       "├─ModuleList: 1-3                        --\n",
       "│    └─HMFeatureBlock: 2-11              --\n",
       "│    │    └─Conv2d: 3-41                 295,040\n",
       "│    │    └─InstanceNorm2d: 3-42         --\n",
       "│    │    └─LeakyReLU: 3-43              --\n",
       "│    │    └─Conv2d: 3-44                 16,512\n",
       "│    └─HMFeatureBlock: 2-12              --\n",
       "│    │    └─Conv2d: 3-45                 73,792\n",
       "│    │    └─InstanceNorm2d: 3-46         --\n",
       "│    │    └─LeakyReLU: 3-47              --\n",
       "│    │    └─Conv2d: 3-48                 4,160\n",
       "│    └─HMFeatureBlock: 2-13              --\n",
       "│    │    └─Conv2d: 3-49                 18,464\n",
       "│    │    └─InstanceNorm2d: 3-50         --\n",
       "│    │    └─LeakyReLU: 3-51              --\n",
       "│    │    └─Conv2d: 3-52                 1,056\n",
       "├─ModuleList: 1-4                        --\n",
       "│    └─HMUpsampleBlock: 2-14             --\n",
       "│    │    └─InstanceNorm2d: 3-53         --\n",
       "│    │    └─LeakyReLU: 3-54              --\n",
       "│    │    └─ConvTranspose2d: 3-55        295,040\n",
       "│    │    └─InstanceNorm2d: 3-56         --\n",
       "│    └─HMUpsampleBlock: 2-15             --\n",
       "│    │    └─InstanceNorm2d: 3-57         --\n",
       "│    │    └─LeakyReLU: 3-58              --\n",
       "│    │    └─ConvTranspose2d: 3-59        73,792\n",
       "│    │    └─InstanceNorm2d: 3-60         --\n",
       "│    └─HMUpsampleBlock: 2-16             --\n",
       "│    │    └─InstanceNorm2d: 3-61         --\n",
       "│    │    └─LeakyReLU: 3-62              --\n",
       "│    │    └─ConvTranspose2d: 3-63        18,464\n",
       "│    │    └─InstanceNorm2d: 3-64         --\n",
       "│    └─HMUpsampleBlock: 2-17             --\n",
       "│    │    └─InstanceNorm2d: 3-65         --\n",
       "│    │    └─LeakyReLU: 3-66              --\n",
       "│    │    └─ConvTranspose2d: 3-67        4,624\n",
       "│    │    └─InstanceNorm2d: 3-68         --\n",
       "├─ModuleList: 1-5                        --\n",
       "│    └─Conv2d: 2-18                      4,160\n",
       "│    └─Conv2d: 2-19                      1,056\n",
       "├─Conv2d: 1-6                            9,248\n",
       "├─Conv2d: 1-7                            33\n",
       "├─LeakyReLU: 1-8                         --\n",
       "├─Softmax: 1-9                           --\n",
       "=================================================================\n",
       "Total params: 2,779,377\n",
       "Trainable params: 2,779,377\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08d50cf-aa6f-4e01-a821-e6527e2d9469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
