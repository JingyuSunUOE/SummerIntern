{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c30a30ac-ab1c-47fd-b725-1e8719756516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# dataset\n",
    "from twaidata.torchdatasets.in_ram_ds import MRISegmentation2DDataset, MRISegmentation3DDataset\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "\n",
    "# model\n",
    "from trustworthai.models.base_models.source_kinet import kiunet, reskiunet, densekiunet, kiunet3d\n",
    "from trustworthai.models.base_models.torchUNet import UNet, UNet3D\n",
    "from trustworthai.models.base_models.deepmedic import DeepMedic\n",
    "\n",
    "# augmentation and pretrain processing\n",
    "from trustworthai.utils.augmentation.standard_transforms import RandomFlip, GaussianBlur, GaussianNoise, \\\n",
    "                                                            RandomResizeCrop, RandomAffine, \\\n",
    "                                                            NormalizeImg, PairedCompose, LabelSelect, \\\n",
    "                                                            PairedCentreCrop, CropZDim\n",
    "# loss function\n",
    "from trustworthai.utils.losses_and_metrics.tversky_loss import TverskyLoss\n",
    "from trustworthai.utils.losses_and_metrics.misc_metrics import IOU\n",
    "from trustworthai.utils.losses_and_metrics.dice import dice, DiceMetric\n",
    "from trustworthai.utils.losses_and_metrics.dice_losses import DiceLoss\n",
    "from trustworthai.utils.losses_and_metrics.power_jaccard_loss import PowerJaccardLoss\n",
    "from torch.nn import BCELoss, MSELoss, BCEWithLogitsLoss\n",
    "\n",
    "# fitter\n",
    "from trustworthai.utils.fitting_and_inference.fitters.basic_lightning_fitter import StandardLitModelWrapper, get_trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# misc\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "064c3a58-1548-4397-bee9-fe0ce81c7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdf\n"
     ]
    }
   ],
   "source": [
    "print(\"sdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e3f18-0236-4cf8-8e26-e76661200c01",
   "metadata": {},
   "source": [
    "### Set the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a450935a-a087-4b81-816b-cd4397db8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3407\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6cea75b-61b8-481c-a1b3-53ad84a254d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\" # blocks gpu 0\n",
    "# torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd568d-9fc6-4868-9d4a-617163ae6e9e",
   "metadata": {},
   "source": [
    "### define datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91099153-5471-4603-8d5e-f8079445aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/disk/scratch/s2208943/ipdis/preprep/out_data/collated/\"\n",
    "wmh_dir = root_dir + \"WMH_challenge_dataset/\"\n",
    "ed_dir = root_dir + \"EdData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74c6f9d9-dd1d-411f-a85b-26a3d82cc48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = [\n",
    "            wmh_dir + d for d in [\"Singapore\", \"Utrecht\", \"GE3T\"]\n",
    "          ] + [\n",
    "            ed_dir + d for d in [\"domainA\", \"domainB\", \"domainC\", \"domainD\"]\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c807e6-40f1-48bc-aed7-d395b569b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation definintion\n",
    "def get_transforms(is_3D):\n",
    "    transforms = [\n",
    "        LabelSelect(label_id=1),\n",
    "        RandomFlip(p=0.5, orientation=\"horizontal\"),\n",
    "        # GaussianBlur(p=0.5, kernel_size=7, sigma=(.1, 1.5)),\n",
    "        # GaussianNoise(p=0.2, mean=0, sigma=0.2),\n",
    "        # RandomAffine(p=0.2, shear=(.1,3.)),\n",
    "        # RandomAffine(p=0.2, degrees=5),\n",
    "        # RandomResizeCrop(p=1., scale=(0.6, 1.), ratio=(3./4., 4./3.))\n",
    "    ]\n",
    "    if not is_3D:\n",
    "        return PairedCompose(transforms)\n",
    "    else:\n",
    "        transforms.append(CropZDim(size=32, minimum=0, maximum=-1))\n",
    "        return PairedCompose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e417fb72-4419-456d-b5ce-e14f0591651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to do train validate test split\n",
    "test_proportion = 0.1\n",
    "validation_proportion = 0.2\n",
    "\n",
    "def train_val_test_split(dataset, val_prop, test_prop, seed):\n",
    "    # I think the sklearn version might be prefereable for determinism and things\n",
    "    # but that involves fiddling with the dataset implementation I think....\n",
    "    size = len(dataset)\n",
    "    test_size = int(test_prop*size) \n",
    "    val_size = int(val_prop*size)\n",
    "    train_size = size - val_size - test_size\n",
    "    train, val, test = random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567c247d-2265-4012-bc80-a92b4f2adec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "# this step is quite slow, all the data is being loaded into memory\n",
    "# datasets_domains = [MRISegmentation2DDataset(root_dir, domain, transforms=None) for domain in domains]\n",
    "datasets_domains = [MRISegmentation3DDataset(root_dir, domain, transforms=get_transforms(is_3D=True)) for domain in domains]\n",
    "\n",
    "# split into train, val test datasets\n",
    "datasets = [train_val_test_split(dataset, validation_proportion, test_proportion, seed) for dataset in datasets_domains]\n",
    "\n",
    "# concat the train val test datsets\n",
    "train_dataset = ConcatDataset([ds[0] for ds in datasets])\n",
    "val_dataset = ConcatDataset([ds[1] for ds in datasets])\n",
    "test_dataset = ConcatDataset([ds[2] for ds in datasets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52f5844a-5c39-47ff-897e-ad36de855931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219, 61, 30)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abfe0f1c-3c76-4069-b698-d7dd2a9c67f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 16, shuffle=False, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71898c27-608d-4ad3-9375-7710bfeb23a5",
   "metadata": {},
   "source": [
    "### setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cf9f593-6e8a-4c26-b9de-0ae024870248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trustworthai.models.base_models.source_kinet import kiunet, reskiunet, densekiunet, kiunet3d\n",
    "# from trustworthai.models.base_models.torchUNet import UNet, UNet3D\n",
    "# from trustworthai.models.base_models.deepmedic import DeepMedic\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 1\n",
    "\n",
    "# fpr 2D models\n",
    "# select a 2D dataloader\n",
    "# for standard UNet\n",
    "#model = UNet(in_channels, out_channels, init_features=32, dropout_p=0.)\n",
    "\n",
    "# model = kiunet(in_channels, out_channels)\n",
    "# model = densekiunet(in_channels, out_channels)\n",
    "# model = reskiunet(in_channels, out_channels)\n",
    "\n",
    "# deepmedic\n",
    "# SCALE_FACTORS = ((3, 3, 3), (1, 1, 1))\n",
    "# FEATURE_MAPS = (30, 30, 40, 40, 40, 40, 50, 50)\n",
    "# FULLY_CONNECTED = (250, 250)\n",
    "# DROPOUT = (.0, .5, .5)\n",
    "# model = DeepMedic(in_channels,\n",
    "#                  out_channels,\n",
    "#                  scale_factors=SCALE_FACTORS,\n",
    "#                  feature_maps=FEATURE_MAPS,\n",
    "#                  fully_connected=FULLY_CONNECTED,\n",
    "#                  dropout=DROPOUT)\n",
    "\n",
    "# # for 3D models\n",
    "# # select a 3D dataloader (note you will have a very few validation/test samples when using just the WMH dataset)\n",
    "# # as there are only 60 scans across the entire dataset.\n",
    "\n",
    "# # for 3D Unet:\n",
    "model = UNet3D(in_channels=3, out_channels=1, init_features=32, dropout_p=0.)\n",
    "#model = UNet_3D(in_channels=3, out_channels=1, init_features=32, dropout_p=0.)\n",
    "# # for kinet\n",
    "# model = kiunet3d(c=in_channels, num_classes=out_channels)\n",
    "\n",
    "\n",
    "# SCALE_FACTORS = ((1,5,5), (1, 3, 3), (1, 1, 1))\n",
    "# FEATURE_MAPS = (30, 30, 40, 40, 40, 40, 50, 50)\n",
    "# FULLY_CONNECTED = (250, 250)\n",
    "# DROPOUT = (.0, .5, .5)\n",
    "# model = DeepMedic(in_channels,\n",
    "#                  out_channels,\n",
    "#                  scale_factors=SCALE_FACTORS,\n",
    "#                  feature_maps=FEATURE_MAPS,\n",
    "#                  fully_connected=FULLY_CONNECTED,\n",
    "#                  dropout=DROPOUT)\n",
    "\n",
    "#loss = DiceLoss()\n",
    "#loss = TverskyLoss()\n",
    "# loss = PowerJaccardLoss()\n",
    "loss = BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea064ef9-44a6-4d93-a454-dc5d9466e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class testUNet3D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=16, dropout_p=0.1, softmax=True):\n",
    "        super().__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet3D._block(in_channels, features, name=\"enc1\", dropout_p=dropout_p)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet3D._block(features, features * 2, name=\"enc2\", dropout_p=dropout_p)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet3D._block(features * 2, features * 4, name=\"enc3\", dropout_p=dropout_p)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet3D._block(features * 4, features * 8, name=\"enc4\", dropout_p=dropout_p)\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet3D._block(features * 8, features * 16, name=\"bottleneck\", dropout_p=dropout_p)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose3d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder4 = UNet3D._block((features * 8) * 2, features * 8, name=\"dec4\", dropout_p=dropout_p)\n",
    "        self.upconv3 = nn.ConvTranspose3d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder3 = UNet3D._block((features * 4) * 2, features * 4, name=\"dec3\", dropout_p=dropout_p)\n",
    "        self.upconv2 = nn.ConvTranspose3d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet3D._block((features * 2) * 2, features * 2, name=\"dec2\", dropout_p=dropout_p)\n",
    "        self.upconv1 = nn.ConvTranspose3d(\n",
    "            features * 2, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet3D._block(features * 2, features, name=\"dec1\", dropout_p=dropout_p)\n",
    "\n",
    "        self.conv = nn.Conv3d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "        self.do_softmax = softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        out = self.conv(dec1)\n",
    "        if self.do_softmax:\n",
    "            return torch.nn.functional.softmax(out)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name, dropout_p):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        name + \"conv1\",\n",
    "                        nn.Conv3d(\n",
    "                            in_channels=in_channels,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm1\", nn.BatchNorm3d(num_features=features)),\n",
    "                    #(name + \"dropout1\", nn.Dropout3d(dropout_p)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (\n",
    "                        name + \"conv3\",\n",
    "                        nn.Conv3d(\n",
    "                            in_channels=features,\n",
    "                            out_channels=features,\n",
    "                            kernel_size=3,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    ),\n",
    "                    (name + \"norm2\", nn.BatchNorm3d(num_features=features)),\n",
    "                    #(name + \"dropout2\", nn.Dropout3d(dropout_p)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c1b81e7-1a7d-4491-9342-f87143ea438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(tensor):\n",
    "    \"\"\"Flattens a given tensor such that the channel axis is first.\n",
    "    The shapes are transformed as follows:\n",
    "       (N, C, D, H, W) -> (C, N * D * H * W)\n",
    "    \"\"\"\n",
    "    # number of channels\n",
    "    C = tensor.size(1)\n",
    "    # new axis order\n",
    "    axis_order = (1, 0) + tuple(range(2, tensor.dim()))\n",
    "    # Transpose: (N, C, D, H, W) -> (C, N, D, H, W)\n",
    "    transposed = tensor.permute(axis_order)\n",
    "    # Flatten: (C, N, D, H, W) -> (C, N * D * H * W)\n",
    "    return transposed.contiguous().view(C, -1)\n",
    "\n",
    "class _AbstractDiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for different implementations of Dice loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight=None, normalization='sigmoid'):\n",
    "        super(_AbstractDiceLoss, self).__init__()\n",
    "        self.register_buffer('weight', weight)\n",
    "        # The output from the network during training is assumed to be un-normalized probabilities and we would\n",
    "        # like to normalize the logits. Since Dice (or soft Dice in this case) is usually used for binary data,\n",
    "        # normalizing the channels with Sigmoid is the default choice even for multi-class segmentation problems.\n",
    "        # However if one would like to apply Softmax in order to get the proper probability distribution from the\n",
    "        # output, just specify `normalization=Softmax`\n",
    "        assert normalization in ['sigmoid', 'softmax', 'none']\n",
    "        if normalization == 'sigmoid':\n",
    "            self.normalization = nn.Sigmoid()\n",
    "        elif normalization == 'softmax':\n",
    "            self.normalization = nn.Softmax(dim=1)\n",
    "        else:\n",
    "            self.normalization = lambda x: x\n",
    "\n",
    "    def dice(self, input, target, weight):\n",
    "        # actual Dice score computation; to be implemented by the subclass\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # get probabilities from logits\n",
    "        input = self.normalization(input)\n",
    "\n",
    "        # compute per channel Dice coefficient\n",
    "        per_channel_dice = self.dice(input, target, weight=self.weight)\n",
    "\n",
    "        # average Dice score across all channels/classes\n",
    "        return 1. - torch.mean(per_channel_dice)\n",
    "\n",
    "class GeneralizedDiceLoss(_AbstractDiceLoss):\n",
    "    \"\"\"Computes Generalized Dice Loss (GDL) as described in https://arxiv.org/pdf/1707.03237.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, normalization='sigmoid', epsilon=1e-6):\n",
    "        super().__init__(weight=None, normalization=normalization)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def dice(self, input, target, weight):\n",
    "        assert input.size() == target.size(), \"'input' and 'target' must have the same shape\"\n",
    "        \n",
    "        input = flatten(input)\n",
    "        target = flatten(target)\n",
    "        target = target.float()\n",
    "\n",
    "        if input.size(0) == 1:\n",
    "            # for GDL to make sense we need at least 2 channels (see https://arxiv.org/pdf/1707.03237.pdf)\n",
    "            # put foreground and background voxels in separate channels\n",
    "            input = torch.cat((input, 1 - input), dim=0)\n",
    "            target = torch.cat((target, 1 - target), dim=0)\n",
    "\n",
    "        # GDL weighting: the contribution of each label is corrected by the inverse of its volume\n",
    "        w_l = target.sum(-1)\n",
    "        w_l = 1 / (w_l * w_l).clamp(min=self.epsilon)\n",
    "        w_l.requires_grad = False\n",
    "\n",
    "        intersect = (input * target).sum(-1)\n",
    "        intersect = intersect * w_l\n",
    "\n",
    "        denominator = (input + target).sum(-1)\n",
    "        denominator = (denominator * w_l).clamp(min=self.epsilon)\n",
    "\n",
    "        return 2 * (intersect.sum() / denominator.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "652fbcfd-72ac-4c87-b4e6-37cd1002ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class StandardLitModelWrapper2(pl.LightningModule):\n",
    "    def __init__(self, model, loss=F.cross_entropy, logging_metrics=None):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        logging metrics are (name, metric function)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.logging_metrics = nn.ModuleList(logging_metrics)\n",
    "\n",
    "        \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x, *args, **kwargs)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # optimizer and schedulers go in the configure optimizers hook\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        lightning automates the training loop, \n",
    "        does epoch, back_tracking, optimizers and schedulers,\n",
    "        and metric reduction.\n",
    "        we just define how we want to process a single batch. \n",
    "        we can optionally pass optimizer_idx if we want to define multiple optimizers within the configure_optimizers\n",
    "        hook, and I presume we can add our own parameters also to functions?\n",
    "        \"\"\"\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        # metrics \n",
    "        if self.logging_metrics != None:\n",
    "            for i, lm in enumerate(self.logging_metrics):\n",
    "                value = lm[1](y_hat, y)\n",
    "                self.log(f\"metric: {i+1}\", value, prog_bar=True)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        note: call trainer.validate() automatically loads the best checkpoint if checkpointing was enabled during fitting\n",
    "        well yes I want to enable checkpointing but will deal with that later.\n",
    "        also it does stuff like model.eval() and torch.no_grad() automatically which is nice.\n",
    "        I will need a custom eval thing to do my dropout estimation but can solve that later too.\n",
    "        \"\"\"\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        val_loss = self.loss(y_hat, y)\n",
    "        \n",
    "        if self.logging_metrics != None:\n",
    "            for i, lm in enumerate(self.logging_metrics):\n",
    "                value = lm[1](y_hat, y)\n",
    "                self.log(f\"metric: {i+1}\", value, prog_bar=True)\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        we would need to directly call this function using the trainer\n",
    "        \"\"\"\n",
    "        X, y = batch\n",
    "        y_hat = self(X)\n",
    "        test_loss = self.loss(y_hat, y)\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        just for making predictions as opposed to collecting metrics etc\n",
    "        note to use this, we just call .predict(dataloader) and it then automates the look\n",
    "        these functions are for a single batch. Nice.\n",
    "        \"\"\"\n",
    "        X, y = batch\n",
    "        pred = self(X)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d139f55-4aa6-4a99-94e6-212d88d6e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = testUNet3D(in_channels=3, out_channels=1, init_features=16, dropout_p=0., softmax=False)\n",
    "#model = ResidualUNet3D(in_channels=3, out_channels=1)\n",
    "loss = GeneralizedDiceLoss(normalization='sigmoid')\n",
    "#loss=BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e60ae5da-3073-4c42-86b1-753fc874b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics=None#[DiceMetric()]\n",
    "# note metrics only work properly when not using distributed stategies I think.\n",
    "# see pytorchlightning guidance on metrics.\n",
    "\n",
    "\n",
    "#model = StandardLitModelWrapper2(model, train_dataloader, test_dataloader, val_dataloader, None, loss, metrics)\n",
    "chpt = \"/disk/scratch/s2208943/results/epoch=103-step=728.ckpt\"\n",
    "model = StandardLitModelWrapper2.load_from_checkpoint(chpt, model=model, \n",
    "                                                      train_dataloader=train_dataloader,\n",
    "                                                      val_dataloader=val_dataloader,\n",
    "                                                      test_dataloader=test_dataloader,\n",
    "                                                      loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "669b2864-efcf-4e7b-bbd8-ea20000f128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"/disk/scratch/s2208943/results/\"\n",
    "strategy = None\n",
    "# strategy = \"deepspeed_stage_2\"\n",
    "# strategy = \"dp\"\n",
    "#strategy = \"deepspeed_stage_2_offload\"\n",
    "\n",
    "accelerator=\"gpu\"\n",
    "devices=1\n",
    "max_epochs=2000\n",
    "precision = 16\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(checkpoint_dir, save_top_k=2, monitor=\"val_loss\")\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=100, verbose=\"False\", mode=\"min\", check_finite=True)\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    accelerator=accelerator,\n",
    "    devices=devices,\n",
    "    max_epochs=max_epochs,\n",
    "    strategy=strategy,\n",
    "    precision=precision,\n",
    "    default_root_dir=checkpoint_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6023f75-a02d-49d6-8b3c-b4b9860c2b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62af758b06f4ea4b9efc587e782611b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss            0.25529372692108154\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.25529372692108154}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08c97b30-a44e-436d-bb9c-1613a2730e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a07e1786764d96b9567d8d8228561d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss            0.28326401114463806\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.28326401114463806}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9b6aa1c-a372-48ff-9b82-14a4fca11c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f37b895c2946f386414f09c74395ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss            0.23687617480754852\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.23687617480754852}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de0353-be84-4764-b89e-8326d6d2b021",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b84231-df5d-4cb4-ae82-2dc586ebb311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /disk/scratch/s2208943/results/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                | Params\n",
      "--------------------------------------------------------\n",
      "0 | model           | testUNet3D          | 5.6 M \n",
      "1 | loss            | GeneralizedDiceLoss | 0     \n",
      "2 | logging_metrics | ModuleList          | 0     \n",
      "--------------------------------------------------------\n",
      "5.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.6 M     Total params\n",
      "11.294    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79226086aff4a3da2f55cad23adfad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.985\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.014 >= min_delta = 0.0. New best score: 0.971\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.009 >= min_delta = 0.0. New best score: 0.962\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "      File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f774a4011f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1358, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1341, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/s2208943/miniconda3/envs/ip/lib/python3.9/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.959\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.954\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.943\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 0.929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.924\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.916\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.899\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.892\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.043 >= min_delta = 0.0. New best score: 0.849\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.845\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "50feb889-67c0-4d89-818a-159ff7e3daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d5def6e09ff4a3599aba04a866517be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss            0.2848452627658844\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.2848452627658844}]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fdb05d2b-5a1f-4a09-bd4c-857904359362",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2774aa44-48ce-4864-8ab3-0fb9f75689c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 32, 224, 160])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f90c4c36-bcc2-469f-bc5f-a079a4f6171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81a29195ac04eec9d88f2d654b9a54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer.predict(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a891fee-bb56-4f0c-8992-35c0f78d1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds0 = torch.sigmoid(preds[0].type(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "03afd763-e7b2-4ec3-8bb3-828734ffdcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 32, 224, 160])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "baa505ec-eb7a-4abf-9614-1bf68f0eb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5df65dd-ccd5-44f1-9004-a37da7a75ae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "islice=10\n",
    "n = 3\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(preds0[n,0,islice,:,:])\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(preds0[n,0,islice,:,:]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53d56eab-1e88-41a0-b4e8-31e533267947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "islice=20\n",
    "n = 4\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(x[0][n,0,islice,:,:])\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(x[1][n,0,islice,:,:])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(preds0[n,0,islice,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93c169f3-b774-4236-afdd-c2044b099c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7701)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice(preds0>0.5, x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b6b4995-a7ee-4608-bae1-466aa3cf74f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "#import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint_dir):\n",
    "    \"\"\"Saves model and training parameters at '{checkpoint_dir}/last_checkpoint.pytorch'.\n",
    "    If is_best==True saves '{checkpoint_dir}/best_checkpoint.pytorch' as well.\n",
    "    Args:\n",
    "        state (dict): contains model's state_dict, optimizer's state_dict, epoch\n",
    "            and best evaluation metric value so far\n",
    "        is_best (bool): if True state contains the best model seen so far\n",
    "        checkpoint_dir (string): directory where the checkpoint are to be saved\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    last_file_path = os.path.join(checkpoint_dir, 'last_checkpoint.pytorch')\n",
    "    torch.save(state, last_file_path)\n",
    "    if is_best:\n",
    "        best_file_path = os.path.join(checkpoint_dir, 'best_checkpoint.pytorch')\n",
    "        shutil.copyfile(last_file_path, best_file_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer=None,\n",
    "                    model_key='model_state_dict', optimizer_key='optimizer_state_dict'):\n",
    "    \"\"\"Loads model and training parameters from a given checkpoint_path\n",
    "    If optimizer is provided, loads optimizer's state_dict of as well.\n",
    "    Args:\n",
    "        checkpoint_path (string): path to the checkpoint to be loaded\n",
    "        model (torch.nn.Module): model into which the parameters are to be copied\n",
    "        optimizer (torch.optim.Optimizer) optional: optimizer instance into\n",
    "            which the parameters are to be copied\n",
    "    Returns:\n",
    "        state\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise IOError(f\"Checkpoint '{checkpoint_path}' does not exist\")\n",
    "\n",
    "    state = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(state[model_key])\n",
    "\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(state[optimizer_key])\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def save_network_output(output_path, output, logger=None):\n",
    "    if logger is not None:\n",
    "        logger.info(f'Saving network output to: {output_path}...')\n",
    "    output = output.detach().cpu()[0]\n",
    "    with h5py.File(output_path, 'w') as f:\n",
    "        f.create_dataset('predictions', data=output, compression='gzip')\n",
    "\n",
    "\n",
    "loggers = {}\n",
    "\n",
    "\n",
    "def get_logger(name, level=logging.INFO):\n",
    "    global loggers\n",
    "    if loggers.get(name) is not None:\n",
    "        return loggers[name]\n",
    "    else:\n",
    "        logger = logging.getLogger(name)\n",
    "        logger.setLevel(level)\n",
    "        # Logging to console\n",
    "        stream_handler = logging.StreamHandler(sys.stdout)\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s [%(threadName)s] %(levelname)s %(name)s - %(message)s')\n",
    "        stream_handler.setFormatter(formatter)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "        loggers[name] = logger\n",
    "\n",
    "        return logger\n",
    "\n",
    "\n",
    "def get_number_of_learnable_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "\n",
    "class RunningAverage:\n",
    "    \"\"\"Computes and stores the average\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.sum = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.count += n\n",
    "        self.sum += value * n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def find_maximum_patch_size(model, device):\n",
    "    \"\"\"Tries to find the biggest patch size that can be send to GPU for inference\n",
    "    without throwing CUDA out of memory\"\"\"\n",
    "    logger = get_logger('PatchFinder')\n",
    "    in_channels = model.in_channels\n",
    "\n",
    "    patch_shapes = [(64, 128, 128), (96, 128, 128),\n",
    "                    (64, 160, 160), (96, 160, 160),\n",
    "                    (64, 192, 192), (96, 192, 192)]\n",
    "\n",
    "    for shape in patch_shapes:\n",
    "        # generate random patch of a given size\n",
    "        patch = np.random.randn(*shape).astype('float32')\n",
    "\n",
    "        patch = torch \\\n",
    "            .from_numpy(patch) \\\n",
    "            .view((1, in_channels) + patch.shape) \\\n",
    "            .to(device)\n",
    "\n",
    "        logger.info(f\"Current patch size: {shape}\")\n",
    "        model(patch)\n",
    "\n",
    "\n",
    "def remove_halo(patch, index, shape, patch_halo):\n",
    "    \"\"\"\n",
    "    Remove `pad_width` voxels around the edges of a given patch.\n",
    "    \"\"\"\n",
    "    assert len(patch_halo) == 3\n",
    "\n",
    "    def _new_slices(slicing, max_size, pad):\n",
    "        if slicing.start == 0:\n",
    "            p_start = 0\n",
    "            i_start = 0\n",
    "        else:\n",
    "            p_start = pad\n",
    "            i_start = slicing.start + pad\n",
    "\n",
    "        if slicing.stop == max_size:\n",
    "            p_stop = None\n",
    "            i_stop = max_size\n",
    "        else:\n",
    "            p_stop = -pad if pad != 0 else 1\n",
    "            i_stop = slicing.stop - pad\n",
    "\n",
    "        return slice(p_start, p_stop), slice(i_start, i_stop)\n",
    "\n",
    "    D, H, W = shape\n",
    "\n",
    "    i_c, i_z, i_y, i_x = index\n",
    "    p_c = slice(0, patch.shape[0])\n",
    "\n",
    "    p_z, i_z = _new_slices(i_z, D, patch_halo[0])\n",
    "    p_y, i_y = _new_slices(i_y, H, patch_halo[1])\n",
    "    p_x, i_x = _new_slices(i_x, W, patch_halo[2])\n",
    "\n",
    "    patch_index = (p_c, p_z, p_y, p_x)\n",
    "    index = (i_c, i_z, i_y, i_x)\n",
    "    return patch[patch_index], index\n",
    "\n",
    "\n",
    "def number_of_features_per_level(init_channel_number, num_levels):\n",
    "    return [init_channel_number * 2 ** k for k in range(num_levels)]\n",
    "\n",
    "\n",
    "class _TensorboardFormatter:\n",
    "    \"\"\"\n",
    "    Tensorboard formatters converts a given batch of images (be it input/output to the network or the target segmentation\n",
    "    image) to a series of images that can be displayed in tensorboard. This is the parent class for all tensorboard\n",
    "    formatters which ensures that returned images are in the 'CHW' format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, name, batch):\n",
    "        \"\"\"\n",
    "        Transform a batch to a series of tuples of the form (tag, img), where `tag` corresponds to the image tag\n",
    "        and `img` is the image itself.\n",
    "        Args:\n",
    "             name (str): one of 'inputs'/'targets'/'predictions'\n",
    "             batch (torch.tensor): 4D or 5D torch tensor\n",
    "        \"\"\"\n",
    "\n",
    "        def _check_img(tag_img):\n",
    "            tag, img = tag_img\n",
    "\n",
    "            assert img.ndim == 2 or img.ndim == 3, 'Only 2D (HW) and 3D (CHW) images are accepted for display'\n",
    "\n",
    "            if img.ndim == 2:\n",
    "                img = np.expand_dims(img, axis=0)\n",
    "            else:\n",
    "                C = img.shape[0]\n",
    "                assert C == 1 or C == 3, 'Only (1, H, W) or (3, H, W) images are supported'\n",
    "\n",
    "            return tag, img\n",
    "\n",
    "        tagged_images = self.process_batch(name, batch)\n",
    "\n",
    "        return list(map(_check_img, tagged_images))\n",
    "\n",
    "    def process_batch(self, name, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class DefaultTensorboardFormatter(_TensorboardFormatter):\n",
    "    def __init__(self, skip_last_target=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.skip_last_target = skip_last_target\n",
    "\n",
    "    def process_batch(self, name, batch):\n",
    "        if name == 'targets' and self.skip_last_target:\n",
    "            batch = batch[:, :-1, ...]\n",
    "\n",
    "        tag_template = '{}/batch_{}/channel_{}/slice_{}'\n",
    "\n",
    "        tagged_images = []\n",
    "\n",
    "        if batch.ndim == 5:\n",
    "            # NCDHW\n",
    "            slice_idx = batch.shape[2] // 2  # get the middle slice\n",
    "            for batch_idx in range(batch.shape[0]):\n",
    "                for channel_idx in range(batch.shape[1]):\n",
    "                    tag = tag_template.format(name, batch_idx, channel_idx, slice_idx)\n",
    "                    img = batch[batch_idx, channel_idx, slice_idx, ...]\n",
    "                    tagged_images.append((tag, self._normalize_img(img)))\n",
    "        else:\n",
    "            # batch has no channel dim: NDHW\n",
    "            slice_idx = batch.shape[1] // 2  # get the middle slice\n",
    "            for batch_idx in range(batch.shape[0]):\n",
    "                tag = tag_template.format(name, batch_idx, 0, slice_idx)\n",
    "                img = batch[batch_idx, slice_idx, ...]\n",
    "                tagged_images.append((tag, self._normalize_img(img)))\n",
    "\n",
    "        return tagged_images\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_img(img):\n",
    "        return np.nan_to_num((img - np.min(img)) / np.ptp(img))\n",
    "\n",
    "\n",
    "def _find_masks(batch, min_size=10):\n",
    "    \"\"\"Center the z-slice in the 'middle' of a given instance, given a batch of instances\n",
    "    Args:\n",
    "        batch (ndarray): 5d numpy tensor (NCDHW)\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for b in batch:\n",
    "        assert b.shape[0] == 1\n",
    "        patch = b[0]\n",
    "        z_sum = patch.sum(axis=(1, 2))\n",
    "        coords = np.where(z_sum > min_size)[0]\n",
    "        if len(coords) > 0:\n",
    "            ind = coords[len(coords) // 2]\n",
    "            result.append(b[:, ind:ind + 1, ...])\n",
    "        else:\n",
    "            ind = b.shape[1] // 2\n",
    "            result.append(b[:, ind:ind + 1, ...])\n",
    "\n",
    "    return np.stack(result, axis=0)\n",
    "\n",
    "\n",
    "def get_tensorboard_formatter(formatter_config):\n",
    "    if formatter_config is None:\n",
    "        return DefaultTensorboardFormatter()\n",
    "\n",
    "    class_name = formatter_config['name']\n",
    "    m = importlib.import_module('pytorch3dunet.unet3d.utils')\n",
    "    clazz = getattr(m, class_name)\n",
    "    return clazz(**formatter_config)\n",
    "\n",
    "\n",
    "def expand_as_one_hot(input, C, ignore_index=None):\n",
    "    \"\"\"\n",
    "    Converts NxSPATIAL label image to NxCxSPATIAL, where each label gets converted to its corresponding one-hot vector.\n",
    "    It is assumed that the batch dimension is present.\n",
    "    Args:\n",
    "        input (torch.Tensor): 3D/4D input image\n",
    "        C (int): number of channels/labels\n",
    "        ignore_index (int): ignore index to be kept during the expansion\n",
    "    Returns:\n",
    "        4D/5D output torch.Tensor (NxCxSPATIAL)\n",
    "    \"\"\"\n",
    "    assert input.dim() == 4\n",
    "\n",
    "    # expand the input tensor to Nx1xSPATIAL before scattering\n",
    "    input = input.unsqueeze(1)\n",
    "    # create output tensor shape (NxCxSPATIAL)\n",
    "    shape = list(input.size())\n",
    "    shape[1] = C\n",
    "\n",
    "    if ignore_index is not None:\n",
    "        # create ignore_index mask for the result\n",
    "        mask = input.expand(shape) == ignore_index\n",
    "        # clone the src tensor and zero out ignore_index in the input\n",
    "        input = input.clone()\n",
    "        input[input == ignore_index] = 0\n",
    "        # scatter to get the one-hot tensor\n",
    "        result = torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
    "        # bring back the ignore_index in the result\n",
    "        result[mask] = ignore_index\n",
    "        return result\n",
    "    else:\n",
    "        # scatter to get the one-hot tensor\n",
    "        return torch.zeros(shape).to(input.device).scatter_(1, input, 1)\n",
    "\n",
    "\n",
    "def convert_to_numpy(*inputs):\n",
    "    \"\"\"\n",
    "    Coverts input tensors to numpy ndarrays\n",
    "    Args:\n",
    "        inputs (iteable of torch.Tensor): torch tensor\n",
    "    Returns:\n",
    "        tuple of ndarrays\n",
    "    \"\"\"\n",
    "\n",
    "    def _to_numpy(i):\n",
    "        assert isinstance(i, torch.Tensor), \"Expected input to be torch.Tensor\"\n",
    "        return i.detach().cpu().numpy()\n",
    "\n",
    "    return (_to_numpy(i) for i in inputs)\n",
    "\n",
    "\n",
    "def create_optimizer(optimizer_config, model):\n",
    "    learning_rate = optimizer_config['learning_rate']\n",
    "    weight_decay = optimizer_config.get('weight_decay', 0)\n",
    "    betas = tuple(optimizer_config.get('betas', (0.9, 0.999)))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=betas, weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def create_lr_scheduler(lr_config, optimizer):\n",
    "    if lr_config is None:\n",
    "        return None\n",
    "    class_name = lr_config.pop('name')\n",
    "    m = importlib.import_module('torch.optim.lr_scheduler')\n",
    "    clazz = getattr(m, class_name)\n",
    "    # add optimizer to the config\n",
    "    lr_config['optimizer'] = optimizer\n",
    "    return clazz(**lr_config)\n",
    "\n",
    "\n",
    "def get_class(class_name, modules):\n",
    "    for module in modules:\n",
    "        m = importlib.import_module(module)\n",
    "        clazz = getattr(m, class_name, None)\n",
    "        if clazz is not None:\n",
    "            return clazz\n",
    "    raise RuntimeError(f'Unsupported dataset class: {class_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "54b9454a-2481-4603-a92c-decf9d221671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def conv3d(in_channels, out_channels, kernel_size, bias, padding):\n",
    "    return nn.Conv3d(in_channels, out_channels, kernel_size, padding=padding, bias=bias)\n",
    "\n",
    "\n",
    "def create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding):\n",
    "    \"\"\"\n",
    "    Create a list of modules with together constitute a single conv layer with non-linearity\n",
    "    and optional batchnorm/groupnorm.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size(int or tuple): size of the convolving kernel\n",
    "        order (string): order of things, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'gcr' -> groupnorm + conv + ReLU\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "            'bcr' -> batchnorm + conv + ReLU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    Return:\n",
    "        list of tuple (name, module)\n",
    "    \"\"\"\n",
    "    assert 'c' in order, \"Conv layer MUST be present\"\n",
    "    assert order[0] not in 'rle', 'Non-linearity cannot be the first operation in the layer'\n",
    "\n",
    "    modules = []\n",
    "    for i, char in enumerate(order):\n",
    "        if char == 'r':\n",
    "            modules.append(('ReLU', nn.ReLU(inplace=True)))\n",
    "        elif char == 'l':\n",
    "            modules.append(('LeakyReLU', nn.LeakyReLU(inplace=True)))\n",
    "        elif char == 'e':\n",
    "            modules.append(('ELU', nn.ELU(inplace=True)))\n",
    "        elif char == 'c':\n",
    "            # add learnable bias only in the absence of batchnorm/groupnorm\n",
    "            bias = not ('g' in order or 'b' in order)\n",
    "            modules.append(('conv', conv3d(in_channels, out_channels, kernel_size, bias, padding=padding)))\n",
    "        elif char == 'g':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            if is_before_conv:\n",
    "                num_channels = in_channels\n",
    "            else:\n",
    "                num_channels = out_channels\n",
    "\n",
    "            # use only one group if the given number of groups is greater than the number of channels\n",
    "            if num_channels < num_groups:\n",
    "                num_groups = 1\n",
    "\n",
    "            assert num_channels % num_groups == 0, f'Expected number of channels in input to be divisible by num_groups. num_channels={num_channels}, num_groups={num_groups}'\n",
    "            modules.append(('groupnorm', nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)))\n",
    "        elif char == 'b':\n",
    "            is_before_conv = i < order.index('c')\n",
    "            if is_before_conv:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(in_channels)))\n",
    "            else:\n",
    "                modules.append(('batchnorm', nn.BatchNorm3d(out_channels)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported layer type '{char}'. MUST be one of ['b', 'g', 'r', 'l', 'e', 'c']\")\n",
    "\n",
    "    return modules\n",
    "\n",
    "\n",
    "class SingleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    Basic convolutional module consisting of a Conv3d, non-linearity and optional batchnorm/groupnorm. The order\n",
    "    of operations can be specified via the `order` parameter\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple):\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
    "        super(SingleConv, self).__init__()\n",
    "\n",
    "        for name, module in create_conv(in_channels, out_channels, kernel_size, order, num_groups, padding=padding):\n",
    "            self.add_module(name, module)\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Sequential):\n",
    "    \"\"\"\n",
    "    A module consisting of two consecutive convolution layers (e.g. BatchNorm3d+ReLU+Conv3d).\n",
    "    We use (Conv3d+ReLU+GroupNorm3d) by default.\n",
    "    This can be changed however by providing the 'order' argument, e.g. in order\n",
    "    to change to Conv3d+BatchNorm3d+ELU use order='cbe'.\n",
    "    Use padded convolutions to make sure that the output (H_out, W_out) is the same\n",
    "    as (H_in, W_in), so that you don't have to crop in the decoder path.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        encoder (bool): if True we're in the encoder path, otherwise we're in the decoder\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "        order (string): determines the order of layers, e.g.\n",
    "            'cr' -> conv + ReLU\n",
    "            'crg' -> conv + ReLU + groupnorm\n",
    "            'cl' -> conv + LeakyReLU\n",
    "            'ce' -> conv + ELU\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, encoder, kernel_size=3, order='gcr', num_groups=8, padding=1):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        if encoder:\n",
    "            # we're in the encoder path\n",
    "            conv1_in_channels = in_channels\n",
    "            conv1_out_channels = out_channels // 2\n",
    "            if conv1_out_channels < in_channels:\n",
    "                conv1_out_channels = in_channels\n",
    "            conv2_in_channels, conv2_out_channels = conv1_out_channels, out_channels\n",
    "        else:\n",
    "            # we're in the decoder path, decrease the number of channels in the 1st convolution\n",
    "            conv1_in_channels, conv1_out_channels = in_channels, out_channels\n",
    "            conv2_in_channels, conv2_out_channels = out_channels, out_channels\n",
    "\n",
    "        # conv1\n",
    "        self.add_module('SingleConv1',\n",
    "                        SingleConv(conv1_in_channels, conv1_out_channels, kernel_size, order, num_groups,\n",
    "                                   padding=padding))\n",
    "        # conv2\n",
    "        self.add_module('SingleConv2',\n",
    "                        SingleConv(conv2_in_channels, conv2_out_channels, kernel_size, order, num_groups,\n",
    "                                   padding=padding))\n",
    "\n",
    "\n",
    "class ExtResNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic UNet block consisting of a SingleConv followed by the residual block.\n",
    "    The SingleConv takes care of increasing/decreasing the number of channels and also ensures that the number\n",
    "    of output channels is compatible with the residual block that follows.\n",
    "    This block can be used instead of standard DoubleConv in the Encoder module.\n",
    "    Motivated by: https://arxiv.org/pdf/1706.00120.pdf\n",
    "    Notice we use ELU instead of ReLU (order='cge') and put non-linearity after the groupnorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, order='cge', num_groups=8, **kwargs):\n",
    "        super(ExtResNetBlock, self).__init__()\n",
    "\n",
    "        # first convolution\n",
    "        self.conv1 = SingleConv(in_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
    "        # residual block\n",
    "        self.conv2 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=order, num_groups=num_groups)\n",
    "        # remove non-linearity from the 3rd convolution since it's going to be applied after adding the residual\n",
    "        n_order = order\n",
    "        for c in 'rel':\n",
    "            n_order = n_order.replace(c, '')\n",
    "        self.conv3 = SingleConv(out_channels, out_channels, kernel_size=kernel_size, order=n_order,\n",
    "                                num_groups=num_groups)\n",
    "\n",
    "        # create non-linearity separately\n",
    "        if 'l' in order:\n",
    "            self.non_linearity = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
    "        elif 'e' in order:\n",
    "            self.non_linearity = nn.ELU(inplace=True)\n",
    "        else:\n",
    "            self.non_linearity = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply first convolution and save the output as a residual\n",
    "        out = self.conv1(x)\n",
    "        residual = out\n",
    "\n",
    "        # residual block\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        out += residual\n",
    "        out = self.non_linearity(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module from the encoder path consisting of the optional max\n",
    "    pooling layer (one may specify the MaxPool kernel_size to be different\n",
    "    than the standard (2,2,2), e.g. if the volumetric data is anisotropic\n",
    "    (make sure to use complementary scale_factor in the decoder path) followed by\n",
    "    a DoubleConv module.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
    "        apply_pooling (bool): if True use MaxPool3d before DoubleConv\n",
    "        pool_kernel_size (int or tuple): the size of the window\n",
    "        pool_type (str): pooling layer: 'max' or 'avg'\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, apply_pooling=True,\n",
    "                 pool_kernel_size=2, pool_type='max', basic_module=DoubleConv, conv_layer_order='gcr',\n",
    "                 num_groups=8, padding=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        assert pool_type in ['max', 'avg']\n",
    "        if apply_pooling:\n",
    "            if pool_type == 'max':\n",
    "                self.pooling = nn.MaxPool3d(kernel_size=pool_kernel_size)\n",
    "            else:\n",
    "                self.pooling = nn.AvgPool3d(kernel_size=pool_kernel_size)\n",
    "        else:\n",
    "            self.pooling = None\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=True,\n",
    "                                         kernel_size=conv_kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups,\n",
    "                                         padding=padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.pooling is not None:\n",
    "            x = self.pooling(x)\n",
    "        x = self.basic_module(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single module for decoder path consisting of the upsampling layer\n",
    "    (either learned ConvTranspose3d or nearest neighbor interpolation) followed by a basic module (DoubleConv or ExtResNetBlock).\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output channels\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel\n",
    "        scale_factor (tuple): used as the multiplier for the image H/W/D in\n",
    "            case of nn.Upsample or as stride in case of ConvTranspose3d, must reverse the MaxPool3d operation\n",
    "            from the corresponding encoder\n",
    "        basic_module(nn.Module): either ResNetBlock or DoubleConv\n",
    "        conv_layer_order (string): determines the order of layers\n",
    "            in `DoubleConv` module. See `DoubleConv` for more info.\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "        upsample (boole): should the input be upsampled\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, conv_kernel_size=3, scale_factor=(2, 2, 2), basic_module=DoubleConv,\n",
    "                 conv_layer_order='gcr', num_groups=8, mode='nearest', padding=1, upsample=True):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        if upsample:\n",
    "            if basic_module == DoubleConv:\n",
    "                # if DoubleConv is the basic_module use interpolation for upsampling and concatenation joining\n",
    "                self.upsampling = InterpolateUpsampling(mode=mode)\n",
    "                # concat joining\n",
    "                self.joining = partial(self._joining, concat=True)\n",
    "            else:\n",
    "                # if basic_module=ExtResNetBlock use transposed convolution upsampling and summation joining\n",
    "                self.upsampling = TransposeConvUpsampling(in_channels=in_channels, out_channels=out_channels,\n",
    "                                                          kernel_size=conv_kernel_size, scale_factor=scale_factor)\n",
    "                # sum joining\n",
    "                self.joining = partial(self._joining, concat=False)\n",
    "                # adapt the number of in_channels for the ExtResNetBlock\n",
    "                in_channels = out_channels\n",
    "        else:\n",
    "            # no upsampling\n",
    "            self.upsampling = NoUpsampling()\n",
    "            # concat joining\n",
    "            self.joining = partial(self._joining, concat=True)\n",
    "\n",
    "        self.basic_module = basic_module(in_channels, out_channels,\n",
    "                                         encoder=False,\n",
    "                                         kernel_size=conv_kernel_size,\n",
    "                                         order=conv_layer_order,\n",
    "                                         num_groups=num_groups,\n",
    "                                         padding=padding)\n",
    "\n",
    "    def forward(self, encoder_features, x):\n",
    "        x = self.upsampling(encoder_features=encoder_features, x=x)\n",
    "        x = self.joining(encoder_features, x)\n",
    "        x = self.basic_module(x)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _joining(encoder_features, x, concat):\n",
    "        if concat:\n",
    "            return torch.cat((encoder_features, x), dim=1)\n",
    "        else:\n",
    "            return encoder_features + x\n",
    "\n",
    "\n",
    "def create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups,\n",
    "                    pool_kernel_size):\n",
    "    # create encoder path consisting of Encoder modules. Depth of the encoder is equal to `len(f_maps)`\n",
    "    encoders = []\n",
    "    for i, out_feature_num in enumerate(f_maps):\n",
    "        if i == 0:\n",
    "            encoder = Encoder(in_channels, out_feature_num,\n",
    "                              apply_pooling=False,  # skip pooling in the firs encoder\n",
    "                              basic_module=basic_module,\n",
    "                              conv_layer_order=layer_order,\n",
    "                              conv_kernel_size=conv_kernel_size,\n",
    "                              num_groups=num_groups,\n",
    "                              padding=conv_padding)\n",
    "        else:\n",
    "            # TODO: adapt for anisotropy in the data, i.e. use proper pooling kernel to make the data isotropic after 1-2 pooling operations\n",
    "            encoder = Encoder(f_maps[i - 1], out_feature_num,\n",
    "                              basic_module=basic_module,\n",
    "                              conv_layer_order=layer_order,\n",
    "                              conv_kernel_size=conv_kernel_size,\n",
    "                              num_groups=num_groups,\n",
    "                              pool_kernel_size=pool_kernel_size,\n",
    "                              padding=conv_padding)\n",
    "\n",
    "        encoders.append(encoder)\n",
    "\n",
    "    return nn.ModuleList(encoders)\n",
    "\n",
    "\n",
    "def create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups, upsample):\n",
    "    # create decoder path consisting of the Decoder modules. The length of the decoder list is equal to `len(f_maps) - 1`\n",
    "    decoders = []\n",
    "    reversed_f_maps = list(reversed(f_maps))\n",
    "    for i in range(len(reversed_f_maps) - 1):\n",
    "        if basic_module == DoubleConv:\n",
    "            in_feature_num = reversed_f_maps[i] + reversed_f_maps[i + 1]\n",
    "        else:\n",
    "            in_feature_num = reversed_f_maps[i]\n",
    "\n",
    "        out_feature_num = reversed_f_maps[i + 1]\n",
    "\n",
    "        # TODO: if non-standard pooling was used, make sure to use correct striding for transpose conv\n",
    "        # currently strides with a constant stride: (2, 2, 2)\n",
    "\n",
    "        _upsample = True\n",
    "        if i == 0:\n",
    "            # upsampling can be skipped only for the 1st decoder, afterwards it should always be present\n",
    "            _upsample = upsample\n",
    "\n",
    "        decoder = Decoder(in_feature_num, out_feature_num,\n",
    "                          basic_module=basic_module,\n",
    "                          conv_layer_order=layer_order,\n",
    "                          conv_kernel_size=conv_kernel_size,\n",
    "                          num_groups=num_groups,\n",
    "                          padding=conv_padding,\n",
    "                          upsample=_upsample)\n",
    "        decoders.append(decoder)\n",
    "    return nn.ModuleList(decoders)\n",
    "\n",
    "\n",
    "class AbstractUpsampling(nn.Module):\n",
    "    \"\"\"\n",
    "    Abstract class for upsampling. A given implementation should upsample a given 5D input tensor using either\n",
    "    interpolation or learned transposed convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, upsample):\n",
    "        super(AbstractUpsampling, self).__init__()\n",
    "        self.upsample = upsample\n",
    "\n",
    "    def forward(self, encoder_features, x):\n",
    "        # get the spatial dimensions of the output given the encoder_features\n",
    "        output_size = encoder_features.size()[2:]\n",
    "        # upsample the input and return\n",
    "        return self.upsample(x, output_size)\n",
    "\n",
    "\n",
    "class InterpolateUpsampling(AbstractUpsampling):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mode (str): algorithm used for upsampling:\n",
    "            'nearest' | 'linear' | 'bilinear' | 'trilinear' | 'area'. Default: 'nearest'\n",
    "            used only if transposed_conv is False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode='nearest'):\n",
    "        upsample = partial(self._interpolate, mode=mode)\n",
    "        super().__init__(upsample)\n",
    "\n",
    "    @staticmethod\n",
    "    def _interpolate(x, size, mode):\n",
    "        return F.interpolate(x, size=size, mode=mode)\n",
    "\n",
    "\n",
    "class TransposeConvUpsampling(AbstractUpsampling):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        in_channels (int): number of input channels for transposed conv\n",
    "            used only if transposed_conv is True\n",
    "        out_channels (int): number of output channels for transpose conv\n",
    "            used only if transposed_conv is True\n",
    "        kernel_size (int or tuple): size of the convolving kernel\n",
    "            used only if transposed_conv is True\n",
    "        scale_factor (int or tuple): stride of the convolution\n",
    "            used only if transposed_conv is True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=None, out_channels=None, kernel_size=3, scale_factor=(2, 2, 2)):\n",
    "        # make sure that the output size reverses the MaxPool3d from the corresponding encoder\n",
    "        upsample = nn.ConvTranspose3d(in_channels, out_channels, kernel_size=kernel_size, stride=scale_factor,\n",
    "                                      padding=1)\n",
    "        super().__init__(upsample)\n",
    "\n",
    "\n",
    "class NoUpsampling(AbstractUpsampling):\n",
    "    def __init__(self):\n",
    "        super().__init__(self._no_upsampling)\n",
    "\n",
    "    @staticmethod\n",
    "    def _no_upsampling(x, size):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b08f009-5514-4ef8-a7a3-471d07128dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Abstract3DUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for standard and residual UNet.\n",
    "    Args:\n",
    "        in_channels (int): number of input channels\n",
    "        out_channels (int): number of output segmentation masks;\n",
    "            Note that that the of out_channels might correspond to either\n",
    "            different semantic classes or to different binary segmentation mask.\n",
    "            It's up to the user of the class to interpret the out_channels and\n",
    "            use the proper loss criterion during training (i.e. CrossEntropyLoss (multi-class)\n",
    "            or BCEWithLogitsLoss (two-class) respectively)\n",
    "        f_maps (int, tuple): number of feature maps at each level of the encoder; if it's an integer the number\n",
    "            of feature maps is given by the geometric progression: f_maps ^ k, k=1,2,3,4\n",
    "        final_sigmoid (bool): if True apply element-wise nn.Sigmoid after the\n",
    "            final 1x1 convolution, otherwise apply nn.Softmax. MUST be True if nn.BCELoss (two-class) is used\n",
    "            to train the model. MUST be False if nn.CrossEntropyLoss (multi-class) is used to train the model.\n",
    "        basic_module: basic model for the encoder/decoder (DoubleConv, ExtResNetBlock, ....)\n",
    "        layer_order (string): determines the order of layers\n",
    "            in `SingleConv` module. e.g. 'crg' stands for Conv3d+ReLU+GroupNorm3d.\n",
    "            See `SingleConv` for more info\n",
    "        num_groups (int): number of groups for the GroupNorm\n",
    "        num_levels (int): number of levels in the encoder/decoder path (applied only if f_maps is an int)\n",
    "        is_segmentation (bool): if True (semantic segmentation problem) Sigmoid/Softmax normalization is applied\n",
    "            after the final convolution; if False (regression problem) the normalization layer is skipped at the end\n",
    "        conv_kernel_size (int or tuple): size of the convolving kernel in the basic_module\n",
    "        pool_kernel_size (int or tuple): the size of the window\n",
    "        conv_padding (int or tuple): add zero-padding added to all three sides of the input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid, basic_module, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, conv_kernel_size=3, pool_kernel_size=2,\n",
    "                 conv_padding=1, **kwargs):\n",
    "        super(Abstract3DUNet, self).__init__()\n",
    "\n",
    "        if isinstance(f_maps, int):\n",
    "            f_maps = number_of_features_per_level(f_maps, num_levels=num_levels)\n",
    "\n",
    "        assert isinstance(f_maps, list) or isinstance(f_maps, tuple)\n",
    "        assert len(f_maps) > 1, \"Required at least 2 levels in the U-Net\"\n",
    "\n",
    "        # create encoder path\n",
    "        self.encoders = create_encoders(in_channels, f_maps, basic_module, conv_kernel_size, conv_padding, layer_order,\n",
    "                                        num_groups, pool_kernel_size)\n",
    "\n",
    "        # create decoder path\n",
    "        self.decoders = create_decoders(f_maps, basic_module, conv_kernel_size, conv_padding, layer_order, num_groups,\n",
    "                                        upsample=True)\n",
    "\n",
    "        # in the last layer a 1×1 convolution reduces the number of output\n",
    "        # channels to the number of labels\n",
    "        self.final_conv = nn.Conv3d(f_maps[0], out_channels, 1)\n",
    "\n",
    "        if is_segmentation:\n",
    "            # semantic segmentation problem\n",
    "            if final_sigmoid:\n",
    "                self.final_activation = nn.Sigmoid()\n",
    "            else:\n",
    "                self.final_activation = nn.Softmax(dim=1)\n",
    "        else:\n",
    "            # regression problem\n",
    "            self.final_activation = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder part\n",
    "        encoders_features = []\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "            # reverse the encoder outputs to be aligned with the decoder\n",
    "            encoders_features.insert(0, x)\n",
    "\n",
    "        # remove the last encoder's output from the list\n",
    "        # !!remember: it's the 1st in the list\n",
    "        encoders_features = encoders_features[1:]\n",
    "\n",
    "        # decoder part\n",
    "        for decoder, encoder_features in zip(self.decoders, encoders_features):\n",
    "            # pass the output from the corresponding encoder and the output\n",
    "            # of the previous decoder\n",
    "            x = decoder(encoder_features, x)\n",
    "\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # apply final_activation (i.e. Sigmoid or Softmax) only during prediction. During training the network outputs logits\n",
    "        if not self.training and self.final_activation is not None:\n",
    "            x = self.final_activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class RUNet3D(Abstract3DUNet):\n",
    "    \"\"\"\n",
    "    3DUnet model from\n",
    "    `\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\"\n",
    "        <https://arxiv.org/pdf/1606.06650.pdf>`.\n",
    "    Uses `DoubleConv` as a basic_module and nearest neighbor upsampling in the decoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
    "        super().__init__(in_channels=in_channels,\n",
    "                                     out_channels=out_channels,\n",
    "                                     final_sigmoid=final_sigmoid,\n",
    "                                     basic_module=DoubleConv,\n",
    "                                     f_maps=f_maps,\n",
    "                                     layer_order=layer_order,\n",
    "                                     num_groups=num_groups,\n",
    "                                     num_levels=num_levels,\n",
    "                                     is_segmentation=is_segmentation,\n",
    "                                     conv_padding=conv_padding,\n",
    "                                     **kwargs)\n",
    "\n",
    "\n",
    "class ResidualUNet3D(Abstract3DUNet):\n",
    "    \"\"\"\n",
    "    Residual 3DUnet model implementation based on https://arxiv.org/pdf/1706.00120.pdf.\n",
    "    Uses ExtResNetBlock as a basic building block, summation joining instead\n",
    "    of concatenation joining and transposed convolutions for upsampling (watch out for block artifacts).\n",
    "    Since the model effectively becomes a residual net, in theory it allows for deeper UNet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=5, is_segmentation=True, conv_padding=1, **kwargs):\n",
    "        super(ResidualUNet3D, self).__init__(in_channels=in_channels,\n",
    "                                             out_channels=out_channels,\n",
    "                                             final_sigmoid=final_sigmoid,\n",
    "                                             basic_module=ExtResNetBlock,\n",
    "                                             f_maps=f_maps,\n",
    "                                             layer_order=layer_order,\n",
    "                                             num_groups=num_groups,\n",
    "                                             num_levels=num_levels,\n",
    "                                             is_segmentation=is_segmentation,\n",
    "                                             conv_padding=conv_padding,\n",
    "                                             **kwargs)\n",
    "\n",
    "\n",
    "class UNet2D(Abstract3DUNet):\n",
    "    \"\"\"\n",
    "    Just a standard 2D Unet. Arises naturally by specifying conv_kernel_size=(1, 3, 3), pool_kernel_size=(1, 2, 2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, final_sigmoid=True, f_maps=64, layer_order='gcr',\n",
    "                 num_groups=8, num_levels=4, is_segmentation=True, conv_padding=1, **kwargs):\n",
    "        if conv_padding == 1:\n",
    "            conv_padding = (0, 1, 1)\n",
    "        super(UNet2D, self).__init__(in_channels=in_channels,\n",
    "                                     out_channels=out_channels,\n",
    "                                     final_sigmoid=final_sigmoid,\n",
    "                                     basic_module=DoubleConv,\n",
    "                                     f_maps=f_maps,\n",
    "                                     layer_order=layer_order,\n",
    "                                     num_groups=num_groups,\n",
    "                                     num_levels=num_levels,\n",
    "                                     is_segmentation=is_segmentation,\n",
    "                                     conv_kernel_size=(1, 3, 3),\n",
    "                                     pool_kernel_size=(1, 2, 2),\n",
    "                                     conv_padding=conv_padding,\n",
    "                                     **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7990b-c052-468d-9e3f-e3f5eb14feea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
